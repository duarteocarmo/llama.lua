*llama.lua.txt*        For Neovim >= 0.9.0       Last change: 2026 February 27

==============================================================================
Table of Contents                                *llama.lua-table-of-contents*

1. llama.lua                                             |llama.lua-llama.lua|
  - Additions                                  |llama.lua-llama.lua-additions|
  - Requirements                            |llama.lua-llama.lua-requirements|
  - Installation                            |llama.lua-llama.lua-installation|
  - Configuration                          |llama.lua-llama.lua-configuration|
  - Commands                                    |llama.lua-llama.lua-commands|
  - Inspiration                              |llama.lua-llama.lua-inspiration|
  - License                                      |llama.lua-llama.lua-license|

==============================================================================
1. llama.lua                                             *llama.lua-llama.lua*

Lua rewrite of llama.vim <https://github.com/ggml-org/llama.vim> for Neovim.
Local LLM-assisted code completion via llama.cpp
<https://github.com/ggml-org/llama.cpp>.


ADDITIONS                                      *llama.lua-llama.lua-additions*

- **Debounced completions** — `auto_fim_debounce_ms` (default 300ms), completions wait until you stop typing
- **Server management** — `server_managed` auto-starts `llama-server`, reuses across instances, stops on exit
- **Filetype filtering** — `filetypes` table for per-filetype control (like copilot.lua)
- **Inline instruct** — visual select + instruction, streams replacement directly into buffer (inspired by gp.nvim), undo with `u`


REQUIREMENTS                                *llama.lua-llama.lua-requirements*

- Neovim ≥ 0.9
- `curl`
- A running llama.cpp <https://github.com/ggml-org/llama.cpp> server, or `llama-server` in PATH with `server_managed = true`


INSTALLATION                                *llama.lua-llama.lua-installation*

lazy.nvim ~

>lua
    {
      "duarteocarmo/llama.lua",
      opts = {},
    }
<

packer.nvim ~

>lua
    use {
      "duarteocarmo/llama.lua",
      config = function()
        require("llama").setup()
      end,
    }
<

vim-plug ~

>vim
    Plug 'duarteocarmo/llama.lua'
    lua require("llama").setup()
<


CONFIGURATION                              *llama.lua-llama.lua-configuration*

>lua
    require("llama").setup({
      endpoint_fim           = "http://127.0.0.1:8012/infill",
      endpoint_inst          = "http://127.0.0.1:8012/v1/chat/completions",
      model_fim              = "",
      model_inst             = "",
      api_key                = "",
      n_prefix               = 256,
      n_suffix               = 64,
      n_predict              = 128,
      stop_strings           = {},
      t_max_prompt_ms        = 500,
      t_max_predict_ms       = 1000,
      show_info              = 2,
      auto_fim               = true,
      max_line_suffix        = 8,
      max_cache_keys         = 250,
      ring_n_chunks          = 16,
      ring_chunk_size        = 64,
      ring_scope             = 1024,
      ring_update_ms         = 1000,
      auto_fim_debounce_ms   = 300,
      server_managed         = false,
      server_args            = { "--fim-qwen-7b-default" },
      filetypes              = {
        ["*"]         = true,
        yaml          = false,
        markdown      = false,
        help          = false,
        gitcommit     = false,
        gitrebase     = false,
        hgcommit      = false,
      },
      keymap_fim_trigger     = "<leader>llf",
      keymap_fim_accept_full = "<Tab>",
      keymap_fim_accept_line = "<S-Tab>",
      keymap_fim_accept_word = "<leader>ll]",
      keymap_inst_trigger    = "<leader>lli",
      keymap_debug_toggle    = "<leader>lld",
      enable_at_startup      = true,
    })
<


COMMANDS                                        *llama.lua-llama.lua-commands*

  Command               Description
  --------------------- -----------------------------
  :LlamaEnable          Enable
  :LlamaDisable         Disable
  :LlamaToggle          Toggle
  :LlamaToggleAutoFim   Toggle auto FIM
  :LlamaInstruct        Instruct (visual selection)
  :LlamaServerStart     Start llama-server
  :LlamaServerStop      Stop llama-server

INSPIRATION                                  *llama.lua-llama.lua-inspiration*

- llama.vim <https://github.com/ggml-org/llama.vim> — the original, all credit to the authors
- copilot.lua <https://github.com/zbirenbaum/copilot.lua> — filetype filtering, non-blocking ghost text
- gp.nvim <https://github.com/Robitx/gp.nvim> — inline replace instruct pattern


LICENSE                                          *llama.lua-llama.lua-license*

MIT

Generated by panvimdoc <https://github.com/kdheepak/panvimdoc>

vim:tw=78:ts=8:noet:ft=help:norl:
